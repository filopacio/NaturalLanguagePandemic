{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ebola_vs_covid_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t1UseOKjrAEk"
      ],
      "authorship_tag": "ABX9TyNZQv6RKr/d0cEj20QV55/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filopacio/_python_4_analytics_nlp_project/blob/main/ebola_vs_covid_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ALzCmxz-Tq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Comparing and contrasting ebola and covid spreading of information on Twitter\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rGKzjKakhZi"
      },
      "source": [
        "## Install and Import useful packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImM-NO9dxHdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b466997c-64b8-4faa-be2f-cf61bf2f86ac"
      },
      "source": [
        "!pip install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "#!pip install twint\n",
        "!pip install nest_asyncio\n",
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import twint \n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import pipeline\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twint\n",
            "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-q5fbb8rb/twint\n",
            "  Running command git clone -q https://github.com/twintproject/twint.git /tmp/pip-install-q5fbb8rb/twint\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q origin/master\n",
            "Requirement already satisfied, skipping upgrade: aiohttp in /root/.local/lib/python3.7/site-packages (from twint) (3.7.4.post0)\n",
            "Requirement already satisfied, skipping upgrade: aiodns in /root/.local/lib/python3.7/site-packages (from twint) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: cchardet in /root/.local/lib/python3.7/site-packages (from twint) (2.1.7)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /root/.local/lib/python3.7/site-packages (from twint) (0.6)\n",
            "Requirement already satisfied, skipping upgrade: elasticsearch in /root/.local/lib/python3.7/site-packages (from twint) (7.13.1)\n",
            "Requirement already satisfied, skipping upgrade: pysocks in /usr/local/lib/python3.7/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from twint) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: aiohttp_socks in /root/.local/lib/python3.7/site-packages (from twint) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: schedule in /root/.local/lib/python3.7/site-packages (from twint) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: geopy in /usr/local/lib/python3.7/dist-packages (from twint) (1.17.0)\n",
            "Requirement already satisfied, skipping upgrade: fake-useragent in /root/.local/lib/python3.7/site-packages (from twint) (0.1.11)\n",
            "Requirement already satisfied, skipping upgrade: googletransx in /root/.local/lib/python3.7/site-packages (from twint) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (21.2.0)\n",
            "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /root/.local/lib/python3.7/site-packages (from aiohttp->twint) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /root/.local/lib/python3.7/site-packages (from aiohttp->twint) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /root/.local/lib/python3.7/site-packages (from aiohttp->twint) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pycares>=4.0.0 in /root/.local/lib/python3.7/site-packages (from aiodns->twint) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: python-socks[asyncio]>=1.2.2 in /root/.local/lib/python3.7/site-packages (from aiohttp_socks->twint) (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.20)\n",
            "Building wheels for collected packages: twint\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.21-cp37-none-any.whl size=38872 sha256=8be3d47c83ea81c0c2f7cc65e91c485ff6782b32c088efbe2bc5af0a2bd03301\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ey9lo7vf/wheels/4f/3b/75/62d04b3b446658ba85401e8868d3cd1d4bc22f17ad755460a6\n",
            "Successfully built twint\n",
            "Installing collected packages: twint\n",
            "  Found existing installation: twint 2.1.21\n",
            "    Uninstalling twint-2.1.21:\n",
            "      Successfully uninstalled twint-2.1.21\n",
            "\u001b[33m  WARNING: The script twint is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed twint-2.1.21\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LFZXCzkkrxF"
      },
      "source": [
        "## Scrape tweets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy1MA24pkzFq"
      },
      "source": [
        "**Query for \"covid\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuxoorJPA_Ge"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jRiVg2nPK1n"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "# Configure\n",
        "c = twint.Config()\n",
        "c.Search = 'covid'\n",
        "c.Lang   = 'en'\n",
        "c.Since  = '2020-01-01'\n",
        "c.Until  = '2021-06-30'\n",
        "c.Pandas = True\n",
        "c.Popular_tweets = True\n",
        "# Run\n",
        "twint.run.Search(c)\n",
        "df_c = twint.storage.panda.Tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_i5lv4Kk1iP"
      },
      "source": [
        "**Query for \"ebola\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPuDSgnJPWed"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "# Configure\n",
        "e = twint.Config()\n",
        "e.Search = 'ebola'\n",
        "e.Lang = 'en'\n",
        "e.Since = '2014-03-01'\n",
        "e.Until = '2015-05-31'\n",
        "e.Pandas = True\n",
        "# Run\n",
        "twint.run.Search(e)\n",
        "df_e = twint.storage.panda.Tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1UseOKjrAEk"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVZbtL_4a52-"
      },
      "source": [
        "I created the clean_text function in order to clean the tweets from noisy characters. \n",
        "\n",
        "items removed: \n",
        "- links\n",
        "- punctuations/special characters \n",
        "- emoticons\n",
        "\n",
        "Before doing so I also put all the texts in lower case.\n",
        "I did not remove alphanumeric words to avoid eliminating words like covid19, covid-19 etc.\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e0GDIT3kXBY"
      },
      "source": [
        "**Text Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_iIFdxEpeWV"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('https://\\S+|www\\.\\S', '', text)      # remove link\n",
        "    text = re.sub(\"['!@#$%^&*()_+<>?:.,;]\" , '', text)  # punctuations/special characters\n",
        "    text = re.sub(re.compile(\"[\"                        # emoticon\n",
        "        u\"\\U0001F600-\\U0001F64F\"  \n",
        "        u\"\\U0001F300-\\U0001F5FF\"  \n",
        "        u\"\\U0001F680-\\U0001F6FF\"  \n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "                           \"]+\", flags=re.UNICODE), '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91PEkqwYpeR7"
      },
      "source": [
        "# SI POTREBBERO CREARE TUTTE LE FUNZIONI DI PRE-PROCESSING  \n",
        "# PER POI TRASFORMARE IL TESTO TUTTO IN UNA VOLTA ALLA FINE\n",
        "\n",
        "cl_tweets = [clean_text(c) for c in df_c[df_c.language == 'en'].tweet]\n",
        "\n",
        "words = [sentence.split() for sentence in cl_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkCr0_NDhG36"
      },
      "source": [
        "After being cleaned, each tweet is splitted into single words. \n",
        "Therefore, 'words' is a list of lists, where each element is a list of separated strings. \n",
        "Now, other pre-processing actions will be performed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA_GSavhiE-C"
      },
      "source": [
        "**Stopwords removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97SsGZcZhF6h"
      },
      "source": [
        "def remove_stopwords(text):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    text = [[i for i in i.split() if i not in stop] for i in text]\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14eVWcYLqIyQ"
      },
      "source": [
        "removed = [remove_stopwords(i) for i in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1x7WR0InIe_"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPL8XiEMm1rn"
      },
      "source": [
        "def lemmatize(text):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(i, pos = 'v') for i in i] for i in text]\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lF6RQF5qbhX"
      },
      "source": [
        "lemmatized = [lemmatize(i) for i in removed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8c2HLdnLtT"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpMKYYT0m_ot"
      },
      "source": [
        "def stem(text):\n",
        "   stemmer = SnowballStemmer(language = 'english')\n",
        "   text = [[stemmer.stem(i) for i in i] for i in text]\n",
        "   return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seMjN-nK-E2c"
      },
      "source": [
        "Final outcome of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWEoecHO7MbZ"
      },
      "source": [
        "def preprocess(text):\n",
        "  text = stem(lemmatize(remove_stopwords(clean_text(text))))\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxyIW0t-Vrc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07x0dMjcqnMX"
      },
      "source": [
        "def word2vec(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  matrix = vectorizer.fit_transform(text)\n",
        "  return matrix "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svw4dG5rEXYD"
      },
      "source": [
        "## Sentiment Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmzzkbBQEcJB"
      },
      "source": [
        "**Polarity of each tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRf9bFLXAxwM"
      },
      "source": [
        "sentiment_classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "sentiment_covid = sentiment_classifier(list(df_c.tweet))\n",
        "sentiment_ebola = sentiment_classifier(list(df_e.tweet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ady63h3GE37-"
      },
      "source": [
        "df_c['sentiment'] = [sentiment_covid[i].get('label') for i in range(len(sentiment_covid))]\n",
        "df_c['polarity'] =  [sentiment_covid[i].get('score') for i in range(len(sentiment_covid))]\n",
        "\n",
        "df_e['sentiment'] = [sentiment_covid[i].get('label') for i in range(len(sentiment_ebola))]\n",
        "df_e['polarity'] =  [sentiment_covid[i].get('score') for i in range(len(sentiment_ebola))]\n",
        "\n",
        "\n",
        "df_c = df_c[df_c.language == 'en'][['date', 'tweet', 'language', 'username', 'nlikes', 'nretweets','sentiment','polarity']].reset_index().drop(df_c.columns[[0]], axis=1)\n",
        "\n",
        "df_e = df_e[df_e.language == 'en'][['date', 'tweet', 'language', 'username', 'nlikes', 'nretweets','sentiment','polarity']].reset_index().drop(df_c.columns[[0]], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR7u3V2rXPmT"
      },
      "source": [
        "def getTweets(user):\n",
        "  nest_asyncio.apply()\n",
        "  u = twint.Config()\n",
        "  u.Username = user \n",
        "  u.Pandas = True\n",
        "  twint.run.Profile(u)\n",
        "  df_t = twint.storage.panda.Tweets_df\n",
        "  return df_t\n",
        "\n",
        "def getInfo(user):\n",
        "  nest_asyncio.apply()\n",
        "  f = twint.Config()\n",
        "  f.Username= user\n",
        "  f.Format = 'user {username} | tweets {tweets} | followers {followers}'\n",
        "  f.Pandas = True\n",
        "  twint.run.Lookup(f)\n",
        "  df = twint.storage.panda.User_df\n",
        "  return df\n",
        "\n",
        "def getSentiment(user):\n",
        "  df = getTweets(user)\n",
        "  sentiment_classifier = pipeline('sentiment-analysis')\n",
        "  sentiment_user = sentiment_classifier(list(df))\n",
        "  df['sentiment'] =  [sentiment_user[i].get('label') for i in range(len(sentiment_user))]\n",
        "  df['polarity']  =  [sentiment_user[i].get('score') for i in range(len(sentiment_user))]\n",
        "  df = df[df.language == 'en'][['date', 'tweet', 'language', 'username', 'nlikes', 'nretweets','sentiment','polarity']].reset_index().drop(df.columns[[0]], axis=1)\n",
        "  return df"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSZZSebKXy7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}