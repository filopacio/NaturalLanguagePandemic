{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ebola_vs_covid_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjy8jCLnWEUoFkLIPumnDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filopacio/_python_4_analytics_nlp_project/blob/main/ebola_vs_covid_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ALzCmxz-Tq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Comparing and contrasting ebola and covid spreading of information on Twitter\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rGKzjKakhZi"
      },
      "source": [
        "## Install and Import useful packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImM-NO9dxHdb"
      },
      "source": [
        "!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "!pip install twint\n",
        "!pip install nest_asyncio\n",
        "import pandas as pd\n",
        "import twint \n",
        "import nest_asyncio\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LFZXCzkkrxF"
      },
      "source": [
        "## Scrape tweets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy1MA24pkzFq"
      },
      "source": [
        "**Query for \"covid\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jRiVg2nPK1n"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "# Configure\n",
        "c = twint.Config()\n",
        "c.Search = \"covid\"\n",
        "c.Limit = 4000\n",
        "c.Lang = 'english'\n",
        "c.Pandas = True\n",
        "# Run\n",
        "twint.run.Search(c)\n",
        "df_c = twint.storage.panda.Tweets_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_i5lv4Kk1iP"
      },
      "source": [
        "**Query for \"ebola\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPuDSgnJPWed"
      },
      "source": [
        "nest_asyncio.apply()\n",
        "# Configure\n",
        "c = twint.Config()\n",
        "c.Limit = 4000\n",
        "c.Search = \"ebola\"\n",
        "c.Pandas = True\n",
        "c.Lang = 'english'\n",
        "# Run\n",
        "twint.run.Search(c)\n",
        "df_e = twint.storage.panda.Tweets_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1UseOKjrAEk"
      },
      "source": [
        "##Â Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVZbtL_4a52-"
      },
      "source": [
        "I created the clean_text function in order to clean the tweets from noisy characters. \n",
        "\n",
        "items removed: \n",
        "- links\n",
        "- punctuations/special characters \n",
        "- emoticons\n",
        "\n",
        "Before doing so I also put all the texts in lower case.\n",
        "I did not remove alphanumeric words to avoid eliminating words like covid19, covid-19 etc.\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e0GDIT3kXBY"
      },
      "source": [
        "**Text Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_iIFdxEpeWV"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('https://\\S+|www\\.\\S', '', text)      # remove link\n",
        "    text = re.sub(\"['!@#$%^&*()_+<>?:.,;]\" , '', text)  # punctuations/special characters\n",
        "    text = re.sub(re.compile(\"[\"                        # emoticon\n",
        "        u\"\\U0001F600-\\U0001F64F\"  \n",
        "        u\"\\U0001F300-\\U0001F5FF\"  \n",
        "        u\"\\U0001F680-\\U0001F6FF\"  \n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "                           \"]+\", flags=re.UNICODE), '', text)\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91PEkqwYpeR7"
      },
      "source": [
        "# SI POTREBBERO CREARE TUTTE LE FUNZIONI DI PRE-PROCESSING  \n",
        "# PER POI TRASFORMARE IL TESTO TUTTO IN UNA VOLTA ALLA FINE\n",
        "\n",
        "cl_tweets = [clean_text(c) for c in df_c[df_c.language == 'en'].tweet]\n",
        "\n",
        "words = [sentence.split() for sentence in cl_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkCr0_NDhG36"
      },
      "source": [
        "After being cleaned, each tweet is splitted into single words. \n",
        "Therefore, 'words' is a list of lists, where each element is a list of separated strings. \n",
        "Now, other pre-processing actions will be performed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA_GSavhiE-C"
      },
      "source": [
        "**Stopwords removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97SsGZcZhF6h"
      },
      "source": [
        "def remove_stopwords(text):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    text = [[i for i in i.split() if i not in stop] for i in text]\n",
        "    return text"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14eVWcYLqIyQ"
      },
      "source": [
        "removed = [remove_stopwords(i) for i in words]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1x7WR0InIe_"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPL8XiEMm1rn"
      },
      "source": [
        "def lemmatize(text):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(i, pos = 'v') for i in i] for i in text]\n",
        "    return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lF6RQF5qbhX"
      },
      "source": [
        "lemmatized = [lemmatize(i) for i in removed]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8c2HLdnLtT"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpMKYYT0m_ot"
      },
      "source": [
        "def stem(text):\n",
        "   stemmer = SnowballStemmer(language = 'english')\n",
        "   text = [[stemmer.stem(i) for i in i] for i in text]\n",
        "   return text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seMjN-nK-E2c"
      },
      "source": [
        "Final outcome of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWEoecHO7MbZ"
      },
      "source": [
        "def preprocess(text):\n",
        "  text = stem(lemmatize(remove_stopwords(clean_text(text))))\n",
        "  return text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxyIW0t-Vrc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07x0dMjcqnMX"
      },
      "source": [
        "def word2vec(text):\n",
        "  vectorizer = CountVectorizer()\n",
        "  matrix = vectorizer.fit_transform(text)\n",
        "  return matrix "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRf9bFLXAxwM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7XCar8p-Xv6"
      },
      "source": [
        ""
      ]
    }
  ]
}